{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Random Forest for Classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.n_classes = len(np.unique(y))\n",
    "        self.tree = self._grow_tree(X, y)\n",
    "\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "\n",
    "        # Stopping criteria\n",
    "        if (self.max_depth is not None and depth >= self.max_depth) or n_labels == 1:\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return {'type': 'leaf', 'value': leaf_value}\n",
    "\n",
    "        # Find the best split\n",
    "        best_split = self._best_split(X, y)\n",
    "        if best_split['impurity'] == 0:\n",
    "            return {'type': 'leaf', 'value': best_split['class']}\n",
    "\n",
    "        # Recursive split\n",
    "        left_indices = np.where(X[:, best_split['feature_index']] <= best_split['threshold'])[0]\n",
    "        right_indices = np.where(X[:, best_split['feature_index']] > best_split['threshold'])[0]\n",
    "\n",
    "        left_subtree = self._grow_tree(X[left_indices], y[left_indices], depth + 1)\n",
    "        right_subtree = self._grow_tree(X[right_indices], y[right_indices], depth + 1)\n",
    "\n",
    "        return {'type': 'split', 'feature_index': best_split['feature_index'],\n",
    "                'threshold': best_split['threshold'],\n",
    "                'left': left_subtree, 'right': right_subtree}\n",
    "\n",
    "    def _best_split(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        best_split = {'impurity': float('inf')}\n",
    "\n",
    "        for feature_index in range(n_features):\n",
    "            feature_values = np.unique(X[:, feature_index])\n",
    "            for threshold in feature_values:\n",
    "                left_indices = np.where(X[:, feature_index] <= threshold)[0]\n",
    "                right_indices = np.where(X[:, feature_index] > threshold)[0]\n",
    "\n",
    "                if len(left_indices) == 0 or len(right_indices) == 0:\n",
    "                    continue\n",
    "\n",
    "                impurity = self._calculate_impurity(y, left_indices, right_indices)\n",
    "                if impurity < best_split['impurity']:\n",
    "                    best_split = {'feature_index': feature_index, 'threshold': threshold,\n",
    "                                  'impurity': impurity, 'class': self._most_common_label(y)}\n",
    "\n",
    "        return best_split\n",
    "\n",
    "    def _calculate_impurity(self, y, left_indices, right_indices):\n",
    "        p_left = len(left_indices) / len(y)\n",
    "        p_right = len(right_indices) / len(y)\n",
    "        return p_left * self._gini_impurity(y[left_indices]) + p_right * self._gini_impurity(y[right_indices])\n",
    "\n",
    "    def _gini_impurity(self, y):\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        p = counts / len(y)\n",
    "        return 1 - np.sum(p ** 2)\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        return Counter(y).most_common(1)[0][0]\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict_sample(x, self.tree) for x in X])\n",
    "\n",
    "    def _predict_sample(self, x, node):\n",
    "        if node['type'] == 'leaf':\n",
    "            return node['value']\n",
    "        if x[node['feature_index']] <= node['threshold']:\n",
    "            return self._predict_sample(x, node['left'])\n",
    "        else:\n",
    "            return self._predict_sample(x, node['right'])\n",
    "\n",
    "class RandomForest:\n",
    "    def __init__(self, n_estimators=100, max_depth=None, max_features=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "        self.estimators = [DecisionTree(max_depth=self.max_depth) for _ in range(self.n_estimators)]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.feature_indices = []\n",
    "\n",
    "        for estimator in self.estimators:\n",
    "            if self.max_features is not None:\n",
    "                indices = np.random.choice(n_features, size=self.max_features, replace=False)\n",
    "            else:\n",
    "                indices = np.arange(n_features)\n",
    "            self.feature_indices.append(indices)\n",
    "            estimator.fit(X[:, indices], y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict_sample(x) for x in X])\n",
    "\n",
    "    def _predict_sample(self, x):\n",
    "        predictions = [estimator.predict([x[indices]])[0] for estimator, indices in zip(self.estimators, self.feature_indices)]\n",
    "        return Counter(predictions).most_common(1)[0][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on a sample classification data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9298245614035088\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Random Forest classifier\n",
    "clf = RandomForest(n_estimators=100, max_depth=5)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Random Forest for Regression task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeRegressor:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._grow_tree(X, y)\n",
    "\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Stopping criteria\n",
    "        if self.max_depth is not None and depth >= self.max_depth:\n",
    "            return {'type': 'leaf', 'value': np.mean(y)}\n",
    "\n",
    "        # Find the best split\n",
    "        best_split = self._best_split(X, y)\n",
    "        if best_split['mse'] == float('inf'):\n",
    "            return {'type': 'leaf', 'value': np.mean(y)}\n",
    "\n",
    "        # Recursive split\n",
    "        left_indices = np.where(X[:, best_split['feature_index']] <= best_split['threshold'])[0]\n",
    "        right_indices = np.where(X[:, best_split['feature_index']] > best_split['threshold'])[0]\n",
    "\n",
    "        left_subtree = self._grow_tree(X[left_indices], y[left_indices], depth + 1)\n",
    "        right_subtree = self._grow_tree(X[right_indices], y[right_indices], depth + 1)\n",
    "\n",
    "        return {'type': 'split', 'feature_index': best_split['feature_index'],\n",
    "                'threshold': best_split['threshold'],\n",
    "                'left': left_subtree, 'right': right_subtree}\n",
    "\n",
    "    def _best_split(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        best_split = {'mse': float('inf')}\n",
    "\n",
    "        for feature_index in range(n_features):\n",
    "            feature_values = np.unique(X[:, feature_index])\n",
    "            for threshold in feature_values:\n",
    "                left_indices = np.where(X[:, feature_index] <= threshold)[0]\n",
    "                right_indices = np.where(X[:, feature_index] > threshold)[0]\n",
    "\n",
    "                if len(left_indices) == 0 or len(right_indices) == 0:\n",
    "                    continue\n",
    "\n",
    "                mse = self._calculate_mse(y[left_indices], y[right_indices])\n",
    "                if mse < best_split['mse']:\n",
    "                    best_split = {'feature_index': feature_index, 'threshold': threshold, 'mse': mse}\n",
    "\n",
    "        return best_split\n",
    "\n",
    "    def _calculate_mse(self, y_left, y_right):\n",
    "        return np.mean((y_left - np.mean(y_left))**2) + np.mean((y_right - np.mean(y_right))**2)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict_tree(x, self.tree) for x in X])\n",
    "\n",
    "    def _predict_tree(self, x, tree):\n",
    "        if tree['type'] == 'leaf':\n",
    "            return tree['value']\n",
    "        if x[tree['feature_index']] <= tree['threshold']:\n",
    "            return self._predict_tree(x, tree['left'])\n",
    "        else:\n",
    "            return self._predict_tree(x, tree['right'])\n",
    "\n",
    "\n",
    "class RandomForestRegressor:\n",
    "    def __init__(self, n_estimators=100, max_depth=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for _ in range(self.n_estimators):\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            indices = np.random.choice(len(X), len(X), replace=True)\n",
    "            tree.fit(X[indices], y[indices])\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.mean([tree.predict(X) for tree in self.trees], axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on a sample regression data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.036820513601105415\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load diabetes dataset\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Random Forest Regressor\n",
    "rf_regressor = RandomForestRegressor(n_estimators=100, max_depth=5)\n",
    "rf_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_regressor.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error (MSE):\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** An MSE of 0.037 from the Random Forest regression model signifies minimal prediction error, implying close alignment between predicted and actual values. We compare this result with the MSE of 2751.53 from the gradient boosting model indicates substantially higher prediction error. Therefore, the Random Forest model outperforms the gradient boosting model in terms of predictive accuracy for this diabetes dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
