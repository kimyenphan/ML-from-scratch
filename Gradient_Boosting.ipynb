{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_boosting_mse(X, y, num_iter, max_depth=1, nu=0.1):\n",
    "    \"\"\"Given X, a array y and num_iter return y_mean and trees \n",
    "   \n",
    "    Input: X, y, num_iter\n",
    "           max_depth\n",
    "           nu (is the shinkage)\n",
    "    Outputs:y_mean, array of trees from DecisionTreeRegression\n",
    "    \"\"\"\n",
    "    trees = []\n",
    "    N, _ = X.shape\n",
    "    y_mean = np.mean(y)\n",
    "    fm = y_mean\n",
    "    for m in range(num_iter):\n",
    "        resid=y-fm\n",
    "        tree = DecisionTreeRegressor(max_depth=max_depth)\n",
    "        tree.fit(X,resid)\n",
    "        trees.append(tree)\n",
    "        fm=fm+nu*tree.predict(X)\n",
    "    return y_mean, trees\n",
    "\n",
    "def gradient_boosting_predict(X, trees, y_mean,  nu=0.1):\n",
    "    \"\"\"Given X, trees, y_mean predict y_hat\n",
    "    \"\"\"\n",
    "    y_hat = np.full(X.shape[0],y_mean)\n",
    "    for tree in trees:\n",
    "        y_hat=y_hat+nu*tree.predict(X)\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test on a sample data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2751.53094468078"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load diabetes dataset\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply Gradient Boosting\n",
    "y_mean, trees = gradient_boosting_mse(X_train, y_train, num_iter=100, max_depth=1, nu=0.1)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = gradient_boosting_predict(X_test, trees, y_mean, nu=0.1)\n",
    "\n",
    "# Calculate MSE\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** MSE measures the average squared difference between actual and predicted values in a regression model. An MSE of 2751.53 on its own doesn't convey good or bad performance without context. It should be compared to the baseline metrics (like the variance of the dataset) and the specific goals of the modeling effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
