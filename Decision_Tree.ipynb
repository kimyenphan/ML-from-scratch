{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionNode:\n",
    "    def __init__(self, col, split, lchild, rchild):\n",
    "        self.col = col\n",
    "        self.split = split\n",
    "        self.lchild = lchild\n",
    "        self.rchild = rchild\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        if x_test[self.col] < self.split: # Make decision based upon x_test[col] and split\n",
    "            return self.lchild.predict(x_test)\n",
    "        return self.rchild.predict(x_test) \n",
    "\n",
    "class LeafNode:\n",
    "    def __init__(self, y, prediction):\n",
    "        \"Create leaf node from y values and prediction; prediction is mean(y) or mode(y)\"\n",
    "        self.n = len(y)\n",
    "        self.prediction = prediction\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        # return prediction\n",
    "        return self.prediction\n",
    "\n",
    "def gini(x):\n",
    "    \"\"\"\n",
    "    Return the gini impurity score for values in y\n",
    "    Assume y = {0,1}\n",
    "    Gini = 1 - sum_i p_i^2 where p_i is the proportion of class i in y\n",
    "    \"\"\"\n",
    "    if len(x) == 0:\n",
    "        return 0\n",
    "    unique_values, counts = np.unique(x, return_counts=True)\n",
    "    p = np.sum((counts/len(x))**2) # proportion of class i in y\n",
    "    return 1 - p \n",
    "\n",
    "    \n",
    "def find_best_split(X, y, loss, min_samples_leaf):\n",
    "    best = (-1, -1, loss(y))\n",
    "    _, n_cols = X.shape\n",
    "    # loop over all features \n",
    "    for col in range(0,n_cols): \n",
    "        feature_vals = X[:,col] # values in current feature\n",
    "        candidates = np.random.choice(feature_vals, size=11) # pick 11 split candidates for current feature\n",
    "        for i in candidates:\n",
    "            # split the data into two groups, left and right, based on the split value, i, in column, col, of X, \n",
    "            y_left = y[feature_vals<i]\n",
    "            y_right = y[feature_vals>=i]\n",
    "\n",
    "            if len(y_left) < min_samples_leaf or len(y_right) < min_samples_leaf: \n",
    "                continue \n",
    "            l = ((len(y_left)*(loss(y_left))) + (len(y_right)*(loss(y_right)))) / (len(y_left) + len(y_right))\n",
    "            if l == 0: \n",
    "                return col, i\n",
    "            if l < best[2]:\n",
    "                best = (col, i, l) # return column number, split value, loss \n",
    "\n",
    "    return best[0], best[1]\n",
    "             \n",
    "class DecisionTree621:\n",
    "    def __init__(self, min_samples_leaf=1, loss=None):\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.loss = loss # loss function; either np.var for regression or gini for classification\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Create a decision tree fit to (X,y) and save as self.root, the root of\n",
    "        our decision tree, for either a classifier or regression.  Leaf nodes for classifiers\n",
    "        predict the most common class (the mode) and regressions predict the average y\n",
    "        for observations in that leaf.\n",
    "\n",
    "        This function is a wrapper around fit_() that just stores the tree in self.root.\n",
    "        \"\"\"\n",
    "        self.root = self.fit_(X, y)\n",
    "\n",
    "\n",
    "    def fit_(self, X, y):\n",
    "        \"\"\"\n",
    "        Recursively create and return a decision tree fit to (X,y) for\n",
    "        either a classification or regression.  This function should call self.create_leaf(X,y)\n",
    "        to create the appropriate leaf node, which will invoke either\n",
    "        RegressionTree621.create_leaf() or ClassifierTree621.create_leaf() depending\n",
    "        on the type of self.\n",
    "\n",
    "        This function is not part of the class \"interface\" and is for internal use, but it\n",
    "        embodies the decision tree fitting algorithm.\n",
    "\n",
    "        (Make sure to call fit_() not fit() recursively.)\n",
    "        \"\"\"\n",
    "        if len(X) < self.min_samples_leaf or len(np.unique(X))==1:\n",
    "            return self.create_leaf(y)\n",
    "        else:\n",
    "            col, split = find_best_split(X,y,self.loss, self.min_samples_leaf) \n",
    "            if col == -1: \n",
    "                return self.create_leaf(y)\n",
    "            \n",
    "            left_child = self.fit_(X[X[:,col]<split], y[X[:,col]<split])\n",
    "            right_child = self.fit_(X[X[:,col]>=split], y[X[:,col]>=split])\n",
    "\n",
    "        return DecisionNode(col,split,left_child,right_child)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        Make a prediction for each record in X_test and return as array.\n",
    "        This method is inherited by RegressionTree621 and ClassifierTree621 and\n",
    "        works for both without modification!\n",
    "        \"\"\"\n",
    "        preds = [self.root.predict(x) for x in X_test]   \n",
    "    \n",
    "        return preds    \n",
    "\n",
    "class RegressionTree621(DecisionTree621):\n",
    "    def __init__(self, min_samples_leaf=1):\n",
    "        super().__init__(min_samples_leaf, loss=np.var)\n",
    "\n",
    "    def score(self, X_test, y_test):\n",
    "        \"Return the R^2 of y_test vs predictions for each record in X_test\"\n",
    "        return r2_score(y_test, self.predict(X_test))\n",
    "\n",
    "    def create_leaf(self, y):\n",
    "        \"\"\"\n",
    "        Return a new LeafNode for regression, passing y and mean(y) to\n",
    "        the LeafNode constructor.\n",
    "        \"\"\"\n",
    "        return LeafNode(y, np.mean(y))\n",
    "\n",
    "\n",
    "class ClassifierTree621(DecisionTree621):\n",
    "    def __init__(self, min_samples_leaf=1):\n",
    "        super().__init__(min_samples_leaf, loss=gini)\n",
    "\n",
    "    def score(self, X_test, y_test):\n",
    "        \"Return the accuracy_score() of y_test vs predictions for each record in X_test\"\n",
    "        return accuracy_score(y_test, self.predict(X_test))\n",
    "\n",
    "    def create_leaf(self, y):\n",
    "        \"\"\"\n",
    "        Return a new LeafNode for classification, passing y and mode(y) to\n",
    "        the LeafNode constructor. Feel free to use scipy.stats to use the mode function.\n",
    "        \"\"\"\n",
    "        return LeafNode(y, stats.mode(y, keepdims=False)[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Apply on a sample data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the model with the Iris dataset, train it, and evaluate its performance\n",
    "model_iris = ClassifierTree621(min_samples_leaf=5)\n",
    "model_iris.fit(X_train, y_train)\n",
    "accuracy_iris = model_iris.score(X_test, y_test)\n",
    "accuracy_iris\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7110266874471061"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Load the California Housing Dataset\n",
    "housing = fetch_california_housing()\n",
    "X, y = housing.data, housing.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the model with the California Housing dataset, train it, and evaluate its performance\n",
    "model = RegressionTree621(min_samples_leaf=20)\n",
    "model.fit(X_train, y_train)\n",
    "accuracy = model.score(X_test, y_test)\n",
    "accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
